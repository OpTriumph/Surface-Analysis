{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justin/anaconda3/envs/ugsnn2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch3d.ops.subdivide_meshes import SubdivideMeshes\n",
    "from pytorch3d.structures.meshes import Meshes\n",
    "\n",
    "import igl\n",
    "from igl import doublearea,grad\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, save_cur=False):\n",
    "    logger.info('==> Saving...')\n",
    "    state = {\n",
    "        'save_path': '',\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "\n",
    "    if save_cur:\n",
    "        state['save_path'] = os.path.join('.', f'ckpt_best_val.pth')\n",
    "        torch.save(state, os.path.join('.', f'ckpt_best_val.pth'))\n",
    "        logger.info(\"Saved in {}\".format(os.path.join('.', f'ckpt_best_val.pth')))\n",
    "    elif epoch % 100 == 0:\n",
    "        state['save_path'] = os.path.join('.', f'ckpt_epoch_{epoch}.pth')\n",
    "        torch.save(state, os.path.join('.', f'ckpt_epoch_{epoch}.pth'))\n",
    "        logger.info(\"Saved in {}\".format(os.path.join('.', f'ckpt_epoch_{epoch}.pth')))\n",
    "    else:\n",
    "        # state['save_path'] = 'current.pth'\n",
    "        # torch.save(state, os.path.join(args.log_dir, 'current.pth'))\n",
    "        print(\"not saving checkpoint\")\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle, gzip\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "# from utils import sparse2tensor, spmatmul\n",
    "def sparse2tensor(m):\n",
    "    \"\"\"\n",
    "    Convert sparse matrix (scipy.sparse) to tensor (torch.sparse)\n",
    "    \"\"\"\n",
    "    assert(isinstance(m, sparse.coo.coo_matrix))\n",
    "    i = torch.LongTensor(np.array([m.row, m.col]))\n",
    "    v = torch.FloatTensor(m.data)\n",
    "    return torch.sparse.FloatTensor(i, v, torch.Size(m.shape))\n",
    "\n",
    "def spmatmul(den, sp):\n",
    "    \"\"\"\n",
    "    den: Dense tensor of shape batch_size x in_chan x #V\n",
    "    1, 40961, 4\n",
    "    sp : Sparse tensor of shape newlen x #V\n",
    "    \"\"\"\n",
    "    # grad_face = spmatmul(input, self.G)\n",
    "    batch_size, in_chan, nv = list(den.size())\n",
    "    new_len = sp.size()[0]\n",
    "    den = den.permute(2, 1, 0).contiguous().view(nv, -1)\n",
    "    res = torch.spmm(sp, den.double()).view(new_len, in_chan, batch_size).contiguous().permute(2, 1, 0)\n",
    "    return res\n",
    "\n",
    "class _MeshConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, l_mesh_file,r_mesh_file, stride=1, bias=True):\n",
    "        assert stride in [1, 2]\n",
    "        super(_MeshConv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.l_ncoeff = 2\n",
    "        self.l_coeffs = Parameter(torch.Tensor(out_channels//2, in_channels//2, self.l_ncoeff))\n",
    "\n",
    "        self.r_ncoeff = 2\n",
    "        self.r_coeffs = Parameter(torch.Tensor(out_channels//2, in_channels//2, self.r_ncoeff))\n",
    "        \n",
    "        self.l_l_coeffs = Parameter(torch.Tensor(in_channels//2, in_channels//2))\n",
    "        self.r_l_coeffs = Parameter(torch.Tensor(in_channels//2, in_channels//2))\n",
    "\n",
    "        self.set_coeffs()\n",
    "        # load mesh file\n",
    "        l_pkl = pickle.load(open(l_mesh_file, \"rb\"))\n",
    "        r_pkl = pickle.load(open(r_mesh_file, \"rb\"))\n",
    "        # l_pkl = pickle.load(open(l_mesh_file, \"rb\"))\n",
    "        \n",
    "        self.l_pkl = l_pkl\n",
    "        self.r_pkl = r_pkl\n",
    "\n",
    "\n",
    "        self.l_nv = self.l_pkl['V'].shape[0]\n",
    "        l_G = sparse2tensor(pickle.load(open(l_mesh_file, \"rb\"))['G'].tocoo())  # gradient matrix V->F, 3#F x #V\n",
    "        # G = torch.tensor(pkl['G'])\n",
    "        l_NS = torch.tensor(l_pkl['NS'], dtype=torch.float32)  # north-south vector field, #F x 3\n",
    "        l_EW = torch.tensor(l_pkl['EW'], dtype=torch.float32)  # east-west vector field, #F x 3\n",
    "        self.register_buffer(\"l_G\", l_G)\n",
    "        self.register_buffer(\"l_NS\", l_NS)\n",
    "        self.register_buffer(\"l_EW\", l_EW)\n",
    "        \n",
    "        self.r_nv = self.r_pkl['V'].shape[0]\n",
    "        r_G = sparse2tensor(pickle.load(open(r_mesh_file, \"rb\"))['G'].tocoo())  # gradient matrix V->F, 3#F x #V\n",
    "        # G = torch.tensor(pkl['G'])\n",
    "        r_NS = torch.tensor(r_pkl['NS'], dtype=torch.float32)  # north-south vector field, #F x 3\n",
    "        r_EW = torch.tensor(r_pkl['EW'], dtype=torch.float32)  # east-west vector field, #F x 3\n",
    "        self.register_buffer(\"r_G\", r_G)\n",
    "        self.register_buffer(\"r_NS\", r_NS)\n",
    "        self.register_buffer(\"r_EW\", r_EW)\n",
    "        \n",
    "\n",
    "        \n",
    "    def set_coeffs(self):\n",
    "        n = self.in_channels * self.l_ncoeff\n",
    "\n",
    "        stdv = 1. / math.sqrt(n)\n",
    "        self.l_coeffs.data.uniform_(-stdv, stdv)\n",
    "        self.r_coeffs.data.uniform_(-stdv, stdv)\n",
    "        self.l_l_coeffs.data.uniform_(-stdv, stdv)\n",
    "        self.r_l_coeffs.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "class MeshConv(_MeshConv):\n",
    "    def __init__(self, in_channels, out_channels, l_mesh_file,r_mesh_file, stride=1, bias=True, agg = False):\n",
    "        super(MeshConv, self).__init__(in_channels, out_channels, l_mesh_file,r_mesh_file, stride, bias)\n",
    "        l_pkl = self.l_pkl\n",
    "        r_pkl = self.r_pkl\n",
    "\n",
    "        if stride == 2:\n",
    "            self.nv_prev = pkl['nv_prev']\n",
    "            L = sparse2tensor(pkl['L'].tocsr()[:self.nv_prev].tocoo()) # laplacian matrix V->V\n",
    "            F2V = sparse2tensor(pkl['F2V'].tocsr()[:self.nv_prev].tocoo())  # F->V, #V x #F\n",
    "        else: # stride == 1\n",
    "\n",
    "            self.l_nv_prev = l_pkl['V'].shape[0]\n",
    "            # L = sparse2tensor(pkl['L'].tocoo())\n",
    "            l_L = l_pkl['L']\n",
    "            l_F2V = sparse2tensor(l_pkl['F2V'].tocoo())\n",
    "\n",
    "            self.r_nv_prev = r_pkl['V'].shape[0]\n",
    "            r_L = r_pkl['L']\n",
    "            # L = sparse2tensor(pkl['L'].tocoo())\n",
    "            r_F2V = sparse2tensor(r_pkl['F2V'].tocoo())\n",
    "        \n",
    "        self.register_buffer(\"l_L\", l_L)\n",
    "        self.register_buffer(\"l_F2V\", l_F2V)\n",
    "        \n",
    "        self.register_buffer(\"r_L\", r_L)\n",
    "        self.register_buffer(\"r_F2V\", r_F2V)\n",
    "        # self.embedding = ConvBNReLU1D(out_channels, out_channels)\n",
    "        # self.C0_1_r = SimplicialConvolution(2, in_channels//2,in_channels//2, variance= 0.01)\n",
    "        # self.C0_1_l = SimplicialConvolution(2, in_channels//2,in_channels//2, variance= 0.01)\n",
    "        self.agg = agg\n",
    "    def forward(self, input):\n",
    "        # compute gradient\n",
    "        self.l_G = self.l_G.double()\n",
    "        # self.l_L = self.l_L.float()\n",
    "        self.l_L = self.l_L.double()\n",
    "\n",
    "        self.l_EW =  self.l_EW.double()\n",
    "        self.l_NS = self.l_NS.double()\n",
    "        self.l_F2V = self.l_F2V.double()\n",
    "        self.r_G = self.r_G.double()\n",
    "        self.r_L = self.r_L.double()\n",
    "        self.r_EW =  self.r_EW.double()\n",
    "        self.r_NS = self.r_NS.double()\n",
    "        self.r_F2V = self.r_F2V.double()\n",
    "\n",
    "        half_dim = input.shape[1]//2\n",
    "        l_grad_face = spmatmul(input[:,:half_dim ,:], self.l_G)\n",
    "        l_grad_face = l_grad_face.view(*(input[:,:half_dim ,:].size()[:2]), 3, -1).permute(0, 1, 3, 2) # gradient, 3 component per face\n",
    "        l_laplacian = spmatmul(input[:,:half_dim ,:], self.l_L)\n",
    "        l_identity = input[:,:half_dim ,:][..., :self.l_nv_prev]\n",
    "        l_feat = [l_identity, l_laplacian]\n",
    "\n",
    "        r_grad_face = spmatmul(input[:,half_dim: ,:], self.r_G)\n",
    "        r_grad_face = r_grad_face.view(*(input[:,half_dim: ,:].size()[:2]), 3, -1).permute(0, 1, 3, 2) # gradient, 3 component per face\n",
    "        r_laplacian = spmatmul(input[:,half_dim: ,:], self.r_L)\n",
    "        r_identity = input[:,half_dim: ,:][..., :self.r_nv_prev]\n",
    "        r_feat = [r_identity, r_laplacian]\n",
    "\n",
    "\n",
    "        out = torch.stack(l_feat, dim=-1)\n",
    "        out2 = torch.stack(r_feat, dim=-1)\n",
    "        out = torch.sum(torch.sum(torch.mul(out.unsqueeze(1), self.l_coeffs.unsqueeze(2)), dim=2), dim=-1)\n",
    "        out2 = torch.sum(torch.sum(torch.mul(out2.unsqueeze(1), self.r_coeffs.unsqueeze(2)), dim=2), dim=-1)\n",
    "        if self.agg:\n",
    "            out = spmatmul(out, self.l_L)\n",
    "            out2 = spmatmul(out2, self.r_L)\n",
    "        \n",
    "        out = torch.cat([out, out2],dim= 1)\n",
    "        out += self.bias.unsqueeze(-1)\n",
    "        return out.float()\n",
    "\n",
    "\n",
    "class MeshConv_transpose(_MeshConv):\n",
    "    def __init__(self, in_channels, out_channels, mesh_file, stride=2, bias=True):\n",
    "        assert(stride == 2)\n",
    "        super(MeshConv_transpose, self).__init__(in_channels, out_channels, mesh_file, stride, bias)\n",
    "        pkl = self.pkl\n",
    "        self.nv_prev = self.pkl['nv_prev']\n",
    "        self.nv_pad = self.nv - self.nv_prev\n",
    "        L = sparse2tensor(pkl['L'].tocoo()) # laplacian matrix V->V\n",
    "        F2V = sparse2tensor(pkl['F2V'].tocoo()) # F->V, #V x #F\n",
    "        self.register_buffer(\"L\", L)\n",
    "        self.register_buffer(\"F2V\", F2V)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # pad input with zeros up to next mesh resolution\n",
    "        ones_pad = torch.ones(*input.size()[:2], self.nv_pad).to(input.device)\n",
    "        input = torch.cat((input, ones_pad), dim=-1)\n",
    "        # compute gradient\n",
    "        grad_face = spmatmul(input, self.G)\n",
    "        grad_face = grad_face.view(*(input.size()[:2]), 3, -1).permute(0, 1, 3, 2) # gradient, 3 component per face\n",
    "        laplacian = spmatmul(input, self.L)\n",
    "        identity = input\n",
    "        grad_face_ew = torch.sum(torch.mul(grad_face, self.EW), keepdim=False, dim=-1)\n",
    "        grad_face_ns = torch.sum(torch.mul(grad_face, self.NS), keepdim=False, dim=-1)\n",
    "        grad_vert_ew = spmatmul(grad_face_ew, self.F2V)\n",
    "        grad_vert_ns = spmatmul(grad_face_ns, self.F2V)\n",
    "\n",
    "        feat = [identity, laplacian, grad_vert_ew, grad_vert_ns]\n",
    "        out = torch.stack(feat, dim=-1)\n",
    "        out = torch.sum(torch.sum(torch.mul(out.unsqueeze(1), self.coeffs.unsqueeze(2)), dim=2), dim=-1)\n",
    "        out += self.bias.unsqueeze(-1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import logging\n",
    "def train(model, device, train_loader, optimizer, epoch, logger):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        data= data.permute(0,2,1)\n",
    "        output = model(data)\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        # loss = F.nll_loss(output, torch.flatten(target))\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    logger.info('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%) \\r'.format(\n",
    "                train_loss, correct, len(train_loader.dataset)-5,\n",
    "                100. * correct / (len(train_loader.dataset)-5)))\n",
    "    \n",
    "\n",
    "def val(model, device, test_loader, logger, best):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data= data.permute(0,2,1)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    logger.info('Val set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%) \\r'.format(\n",
    "                test_loss, correct, len(test_loader.dataset),\n",
    "                100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    best.append(correct / len(test_loader.dataset))\n",
    "    logger.info('Best Val Accuracy:({:.4f}%) \\r'.format(100. * max(best)))\n",
    "    acc = correct / len(test_loader.dataset)\n",
    "    return best, acc, test_loss\n",
    "\n",
    "def test(model, device, test_loader, logger, best, append = False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data= data.permute(0,2,1)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            \n",
    "            # Calculate the confusion matrix\n",
    "            conf_matrix = confusion_matrix(torch.flatten(target).cpu().numpy(), torch.flatten(pred).cpu().numpy())\n",
    "            # Extract values from the confusion matrix\n",
    "            true_negatives, false_positives, false_negatives, true_positives = conf_matrix.ravel()\n",
    "            # Calculate sensitivity (recall)\n",
    "            sensitivity = true_positives / (true_positives + false_negatives)\n",
    "            specifity = true_negatives / (true_negatives + false_positives)\n",
    "\n",
    "            # Print the confusion matrix and sensitivity\n",
    "            print(\"Sensitivity (Recall):\", sensitivity)\n",
    "            print(\"Sensitivity (Recall):\", specifity)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = correct / len(test_loader.dataset)\n",
    "    \n",
    "    # if acc >= 0.89: \n",
    "    logger.info('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%) \\r'.format(\n",
    "                    test_loss, correct, len(test_loader.dataset),\n",
    "                    100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    if append:\n",
    "            best.append(correct / len(test_loader.dataset))\n",
    "            logger.info('Best Test Accuracy:({:.4f}%) \\r'.format(100. * max(best)))\n",
    "            \n",
    "    return best, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys; \n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class MaxPool(nn.Module):\n",
    "    def __init__(self, level, nv_prev):\n",
    "        super().__init__()\n",
    "        self.level = level\n",
    "        self.nv_prev = nv_prev\n",
    "\n",
    "        if self.level > 0:\n",
    "\n",
    "            neihboring_patches_file = os.path.join(\"/home/justin/Mesh/Matrices_out/l_%d_patches.npy\" % level)\n",
    "            self.neihboring_patches = np.load(neihboring_patches_file)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tmp = x[...,:self.nv_prev]\n",
    "        out, _ = torch.max(tmp[:, :, self.neihboring_patches], -1)\n",
    "\n",
    "        return out\n",
    "\n",
    "class DownSamp(nn.Module):\n",
    "    def __init__(self, nv_prev, lvl, in_chan,hami,e2n = False, down = True):\n",
    "        super().__init__()\n",
    "        self.nv_prev = nv_prev\n",
    "        self.lvl = lvl + 1\n",
    "        self.e2n = e2n\n",
    "        if e2n:\n",
    "            if down:\n",
    "                #face-lvel \n",
    "                self.e2f = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "                self.ff = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "                self.f = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "\n",
    "                #edge-level\n",
    "                self.ee1 = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "                self.ee2 = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "                self.f2e = nn.Linear(2*in_chan, 2*in_chan, bias=True)\n",
    "                self.n2e = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "                self.e = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "\n",
    "                #node-level\n",
    "                self.nn = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "                self.e22n = nn.Linear(2*in_chan, 2*in_chan, bias=True)\n",
    "                self.n = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "\n",
    "\n",
    "                self.f_bn1 = nn.BatchNorm1d(2*in_chan)\n",
    "                self.e_bn1 = nn.BatchNorm1d(2*in_chan)\n",
    "                self.n_bn1 = nn.BatchNorm1d(2*in_chan)\n",
    "            else:\n",
    "                self.e2f = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.ff = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.f = nn.Linear(in_chan, in_chan, bias=True)\n",
    "\n",
    "                #edge-level\n",
    "                self.ee1 = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.ee2 = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.f2e = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.n2e = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.e = nn.Linear(in_chan, in_chan, bias=True)\n",
    "\n",
    "                #node-level\n",
    "                self.nn = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.e22n = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.n = nn.Linear(in_chan, in_chan, bias=True)\n",
    "\n",
    "                self.affine_a = nn.Parameter(torch.ones([1,in_chan, 1]))\n",
    "                self.affine_b = nn.Parameter(torch.zeros([1,in_chan, 1]))\n",
    "                \n",
    "                self.f_bn1 = nn.BatchNorm1d(in_chan)\n",
    "                self.e_bn1 = nn.BatchNorm1d(in_chan)\n",
    "                self.n_bn1 = nn.BatchNorm1d(in_chan)\n",
    "        \n",
    "        self.hami = hami\n",
    "        self.ch = in_chan\n",
    "        self.down = down\n",
    "    def forward(self, x):\n",
    "        if self.e2n is True:\n",
    "            # l = pickle.load(open('/home/justin/Mesh/surfaces/matrix/{}_lvl_{}.pkl'.format(self.hami,self.lvl), \"rb\"))\n",
    "            l = pickle.load(open('/home/justin/Mesh/Matrices_out/{}_lvl_{}.pkl'.format(self.hami,self.lvl), \"rb\"))\n",
    "            E = l['E']\n",
    "            Fa = l['F']\n",
    "            B1 = l[\"B1\"].cuda()\n",
    "            B1t = l[\"B1t\"].cuda()\n",
    "            B2 = l[\"B2\"].cuda()\n",
    "            B2t = l[\"B2t\"].cuda()\n",
    "            L1d = l['L1d'].cuda()\n",
    "            L1u = l['L1u'].cuda()\n",
    "            L2d = l['L2d'].cuda()\n",
    "            L0u = l['L0u'].cuda()\n",
    "\n",
    "            #PE node\n",
    "            batch_size = x.shape[0]\n",
    "            edge_features_list = []\n",
    "            for b in range(batch_size):\n",
    "\n",
    "                concatenated_features = ( 0.5 * x[b,:,E[:,0]].unsqueeze(0) ) +  ( 0.5 * x[b,:,E[:,1]].unsqueeze(0) )\n",
    "                concatenated_features = concatenated_features.squeeze()\n",
    "                edge_features_list.append(concatenated_features)\n",
    "\n",
    "            edge_features = torch.stack(edge_features_list, dim=0)\n",
    "            # edge_PE = self.pe3(torch.stack(PE_list, dim = 0).permute(0,2,1)).permute(0,2,1)\n",
    "\n",
    "            face_features_list = []\n",
    "            PE_list = []\n",
    "            Fa= torch.tensor(Fa)\n",
    "            for b in range(batch_size):\n",
    "\n",
    "                concatenated_features =( 0.3 * x[b,:,Fa[:,0]].unsqueeze(0) ) + (0.3 * x[b,:,Fa[:,1]].unsqueeze(0)) + (0.3 * x[b,:,Fa[:,2]].unsqueeze(0))\n",
    "                concatenated_features = concatenated_features.squeeze()\n",
    "                face_features_list.append(concatenated_features)\n",
    "\n",
    "            face_features = torch.stack(face_features_list, dim=0)\n",
    "            # face_PE = self.pe4(torch.stack(PE_list, dim = 0).permute(0,2,1)).permute(0,2,1)            #face-level\n",
    "            FF  =self.ff(face_features.permute(0,2,1)).permute(0,2,1)\n",
    "            FF = spmatmul(FF, L2d.double())\n",
    "\n",
    "            E2F = self.e2f(edge_features.permute(0,2,1)).permute(0,2,1)\n",
    "            E2F = spmatmul(E2F, B2t.double())\n",
    "        \n",
    "            face_features = F.relu(self.f_bn1((FF + E2F +self.f(face_features.permute(0,2,1)).permute(0,2,1)).float()))\n",
    "            #edge-level\n",
    "            EE1  =self.ee1(edge_features.permute(0,2,1)).permute(0,2,1)\n",
    "            EE1 = spmatmul(EE1, L1d.double())\n",
    "            \n",
    "            EE2  =self.ee2(edge_features.permute(0,2,1)).permute(0,2,1)\n",
    "            EE2 = spmatmul(EE2, L1u.double())\n",
    "            \n",
    "            F2E = self.f2e(face_features.permute(0,2,1)).permute(0,2,1)\n",
    "            F2E= spmatmul(F2E, B2.double())\n",
    "\n",
    "            N2E = self.n2e(x.permute(0,2,1)).permute(0,2,1)\n",
    "            N2E= spmatmul(N2E, B1t.double())\n",
    "\n",
    "            edge_features =F.relu(self.e_bn1((EE1 + EE2 + F2E + N2E +self.e(edge_features.permute(0,2,1)).permute(0,2,1)).float()))\n",
    "\n",
    "            #node-level\n",
    "            NN  =self.nn(x.permute(0,2,1)).permute(0,2,1)\n",
    "            NN = spmatmul(NN, L0u.double())\n",
    "\n",
    "            E2N = self.e22n(edge_features.permute(0,2,1)).permute(0,2,1)\n",
    "            E2N = spmatmul(E2N, B1.double())\n",
    "\n",
    "            if self.down:\n",
    "                final_node = F.relu(self.n_bn1((NN + E2N + self.n(x.permute(0,2,1)).permute(0,2,1)).float()) )\n",
    "            else:\n",
    "                final_node = F.relu(self.n_bn1((NN + E2N + self.n(x.permute(0,2,1)).permute(0,2,1)).float())  + self.affine_a * x + self.affine_b)\n",
    "\n",
    "            return final_node[...,:self.nv_prev].float() \n",
    "\n",
    "        return x[..., :self.nv_prev] \n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_chan, neck_chan, out_chan, level, coarsen, mesh_folder, e2n = False):\n",
    "        super().__init__()\n",
    "\n",
    "        in_chan = int(in_chan//2)\n",
    "        neck_chan = int(neck_chan//2)\n",
    "        out_chan = int(out_chan//2)\n",
    "\n",
    "        l = level-1 if coarsen else level\n",
    "        self.coarsen = coarsen\n",
    "        mesh_file = os.path.join(mesh_folder, \"left_icosphere_{}.pkl\".format(l))\n",
    "        mesh_file2 = os.path.join(mesh_folder, \"right_icosphere_{}.pkl\".format(l))\n",
    "        self.conv2 = MeshConv(neck_chan, neck_chan, mesh_file,mesh_file2, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.nv_prev = self.conv2.l_nv_prev\n",
    "\n",
    "        self.down1 = DownSamp(self.nv_prev,l,in_chan,'l',e2n, down = False)\n",
    "        self.down11 = DownSamp(self.nv_prev,l-1,in_chan,'l',e2n, down = False)\n",
    "        self.down111 = DownSamp(self.nv_prev,l-1,in_chan,'l',e2n)\n",
    "        self.down3 = DownSamp(self.nv_prev,l,in_chan,'r',e2n, down = False)\n",
    "        self.down33 = DownSamp(self.nv_prev,l-1,in_chan,'r',e2n, down = False)\n",
    "        self.down333 = DownSamp(self.nv_prev,l-1,in_chan,'r',e2n)\n",
    "\n",
    "        self.diff_chan = (in_chan != out_chan)\n",
    "\n",
    "        if coarsen:\n",
    "            self.seq1 = nn.Sequential(self.down1, self.down11,self.down111)\n",
    "        else:\n",
    "            self.seq1 = nn.Sequential(self.conv1, self.bn1, self.relu, \n",
    "                                      self.conv2, self.bn2, self.relu, \n",
    "                                      self.conv3, self.bn3)\n",
    "\n",
    "        if coarsen:\n",
    "            self.r_seq1 = nn.Sequential(self.down3, self.down33,self.down333)\n",
    "        else:\n",
    "            self.r_seq1 = nn.Sequential(self.r_conv1, self.r_bn1, self.r_relu, \n",
    "                                      self.r_conv2, self.r_bn2, self.r_relu, \n",
    "                                      self.r_conv3, self.r_bn3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        dim= x.shape[1]\n",
    "        r_x, l_x  = x[:,int(dim/2):,:],x[:,:int(dim/2),:]\n",
    "        cut = l_x.shape[-1]\n",
    "        x1 = self.seq1(l_x)\n",
    "        r_x1 = self.r_seq1(r_x)\n",
    "\n",
    "        x1 = torch.cat([x1,r_x1],axis = 1)\n",
    "\n",
    "        out = x1 \n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "class ConvBNReLU1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, bias=True, activation='relu'):\n",
    "        super(ConvBNReLU1D, self).__init__()\n",
    "        # self.act = nn.ReLU(inplace=True)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, bias=bias),\n",
    "            # nn.BatchNorm1d(out_channels),\n",
    "            # self.act\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def make_weights(labels, nclasses):\n",
    "    labels = np.array(labels) \n",
    "    weight_arr = np.zeros_like(labels) \n",
    "    \n",
    "    _, counts = np.unique(labels, return_counts=True) \n",
    "    for cls in range(nclasses):\n",
    "        weight_arr = np.where(labels == cls, 1/counts[cls], weight_arr) \n",
    "\n",
    " \n",
    "    return weight_arr\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, mesh_folder, feat=16, nclasses=10):\n",
    "        super().__init__()\n",
    "        mf_l = os.path.join(mesh_folder, \"left_icosphere_6.pkl\")\n",
    "        mf_r = os.path.join(mesh_folder, \"right_icosphere_6.pkl\")\n",
    "\n",
    "        self.in_conv = MeshConv(2, 2*feat, l_mesh_file=mf_l,r_mesh_file=mf_r, stride=1, agg = False)\n",
    "        self.in_bn = nn.BatchNorm1d(2*feat)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.in_block = nn.Sequential(self.in_conv, self.in_bn, self.relu)\n",
    "\n",
    "        self.maxp = MaxPool(5, 10242)\n",
    "\n",
    "        L_relu = nn.LeakyReLU()\n",
    "        self.block2 = ResBlock(in_chan=2*feat, neck_chan= 4*feat, out_chan=4*feat, level=5, coarsen=True, mesh_folder=mesh_folder, e2n = True)\n",
    "        self.block3 = ResBlock(in_chan=4*feat, neck_chan= 8*feat, out_chan=8*feat, level=4, coarsen=True, mesh_folder=mesh_folder, e2n = True)\n",
    "        self.block4 = ResBlock(in_chan=8*feat, neck_chan=16*feat, out_chan=16*feat, level=3, coarsen=True, mesh_folder=mesh_folder, e2n = True)\n",
    "        self.block5 = ResBlock(in_chan=16*feat, neck_chan=32*feat, out_chan=32*feat, level=2, coarsen=True, mesh_folder=mesh_folder, e2n = True)\n",
    "\n",
    "        self.avg = nn.AvgPool1d(kernel_size=self.block5.nv_prev) # output shape batch x channels x 1\n",
    "        # self.out_layer = nn.Linear(64*feat, 16*feat)\n",
    "        # self.out_layer2 = nn.Linear(16*feat, 2)\n",
    "        self.D = nn.Sequential(nn.Linear(32*feat,32*feat),L_relu,nn.Linear(32*feat,8*feat),L_relu,nn.Linear(8*feat,2)) #nn.Softmax(dim=0) for multi-class\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.in_block(x)\n",
    "        # x = x[...,:10242]\n",
    "        x = self.maxp(x).float() \n",
    "\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        # x = self.block6(x)\n",
    "\n",
    "        x_l = torch.squeeze(self.avg(x[:, :x.shape[1]//2,: ]))\n",
    "        x_r = torch.squeeze(self.avg(x[:, x.shape[1]//2:,: ]))\n",
    "        x= torch.cat([x_l,x_r], dim = 1)\n",
    "        # x=x_l+x_r\n",
    "        x = self.D(x)\n",
    "        # x = F.dropout(x,training=self.training)\n",
    "        # x = self.out_layer(x)\n",
    "        # x = F.dropout(x,training=self.training)\n",
    "        # x = self.out_layer2(x)\n",
    "        # return F.log_softmax(x, dim=1)\n",
    "        return torch.sigmoid(x)\n",
    "        # return x\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "random_seed = 54\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "\n",
    "\n",
    "# X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "# KFold(n_splits = k, shuffle = True, random_state = 1)\n",
    "\n",
    "k = 5     # k-cross validation에서 k의 값이 5\n",
    "test_bests = []\n",
    "val_test= []\n",
    "# skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=65)\n",
    "skf = StratifiedShuffleSplit(n_splits=k, test_size = 0.2, train_size = 0.8, random_state = 42)\n",
    "for i, (trainval_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "    # Split data into training and test sets for the current fold\n",
    "    X_trainval, X_test = X[trainval_idx], X[test_idx]\n",
    "    y_trainval, y_test = y[trainval_idx], y[test_idx]\n",
    "    \n",
    "    # Further split the training set into training and validation sets\n",
    "    print(len(y_test), len(X))\n",
    "    val_po = (len(X) * 0.1) / (len(X) * 0.8)\n",
    "    train_po = 1 - val_po\n",
    "    \n",
    "    skf = StratifiedShuffleSplit(n_splits=1, test_size = val_po, train_size = train_po)\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(skf.split(X_trainval, y_trainval)):\n",
    "        X_train, X_val = X_trainval[train_idx], X_trainval[val_idx]\n",
    "        y_train, y_val = y_trainval[train_idx], y_trainval[val_idx]\n",
    "\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    train_list = []\n",
    "    for i,j in enumerate(list(X_train)):\n",
    "        train_list.append([j,list(y_train)[i]])\n",
    "\n",
    "    val_list = []\n",
    "    for i,j in enumerate(list(X_val)):\n",
    "        val_list.append([j,list(y_val)[i]])\n",
    "\n",
    "\n",
    "    test_list = []\n",
    "    for i,j in enumerate(list(X_test)):\n",
    "        test_list.append([j,list(y_test)[i]])\n",
    "\n",
    "    print(len(train_list), len(val_list), len(test_list))\n",
    "\n",
    "    weights = make_weights(y_train, 2)\n",
    "    weights = torch.DoubleTensor(weights)\n",
    "    print(weights)\n",
    "    sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "    train_loader = DataLoader(train_list, batch_size = 32, shuffle = False, sampler = sampler)\n",
    "    val_loader = DataLoader(val_list, batch_size = len(val_list))\n",
    "    test_loader = DataLoader(test_list, batch_size = len(test_list))\n",
    "\n",
    "\n",
    "    # train_loader\n",
    "    train_features, train_labels = next(iter(train_loader))\n",
    "    print(f\"Feature batch shape: {train_features.size()}\")\n",
    "    print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "    val_features, val_labels = next(iter(val_loader))\n",
    "    print(f\"Feature batch shape: {val_features.size()}\")\n",
    "    print(f\"Labels batch shape: {val_labels.size()}\")\n",
    "\n",
    "    test_features, test_labels = next(iter(test_loader))\n",
    "    print(f\"Feature batch shape: {test_features.size()}\")\n",
    "    print(f\"Labels batch shape: {test_labels.size()}\")\n",
    "\n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "    import torch.optim as optim\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "    %cd /home/justin/Mesh\n",
    "    #Model parameters: template path, feature dimension, output class number\n",
    "    model = Model(\".\",16, 2)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    # optimizer = optim.SGD(model.parameters(), lr = 0.05, momentum = 0.9)\n",
    "    # scheduler = CosineAnnealingLR(optimizer,\n",
    "                                    # T_max = 100, # Maximum number of iterations.\n",
    "                                    # eta_min = 0.01 / 50) # Minimum learning rate.\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.7)\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3,9,12,15,18], gamma=0.6)\n",
    "    logger = logging.getLogger(\"train\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.handlers = []\n",
    "    ch = logging.StreamHandler()\n",
    "    logger.addHandler(ch)\n",
    "    fh = logging.FileHandler(os.path.join('.', \"0911last_decay.txt\"))\n",
    "    logger.addHandler(fh)\n",
    "    test_acc = None\n",
    "    logger.info(\"{} paramerters in total\".format(sum(x.numel() for x in model.parameters())))\n",
    "    best = []\n",
    "    tbest = []\n",
    "    best_loss = 0\n",
    "    tolerance = 0\n",
    "\n",
    "    for epoch in range(1, 50 + 1):\n",
    "            logger.info(\"[Epoch {}]\".format(epoch))\n",
    "            train(model, device, train_loader, optimizer, epoch, logger)\n",
    "            #validate every 100 epoch\n",
    "            # if epoch % 50 == 0:\n",
    "            best, acc, val_loss= val( model, device, val_loader, logger,best)\n",
    "            # if test_acc is not None:\n",
    "                    #   print(test_acc)\n",
    "            if acc >= best_loss:\n",
    "                best_loss = acc\n",
    "                tolerance = 0\n",
    "                # save_checkpoint(epoch,model, optimizer, scheduler, save_cur=True)\n",
    "\n",
    "            else:\n",
    "                tolerance += 1\n",
    "                # save_checkpoint(epoch,model, optimizer, scheduler, save_cur=True)\n",
    "            print(tolerance)\n",
    "            if acc == max(best):\n",
    "                    # save_checkpoint(epoch,model, optimizer, scheduler, save_cur=True)\n",
    "                    t_best, test_acc= test( model, device, test_loader, logger, tbest, append = True)\n",
    "                    vals = [acc, test_acc]\n",
    "\n",
    "\n",
    "            else:\n",
    "                    t_best, test_acc= test( model, device, test_loader, logger, tbest, append = False)\n",
    "            if epoch <= 8:\n",
    "                tolerance = 0\n",
    "                \n",
    "            if tolerance <= 8:\n",
    "                # break\n",
    "                save_checkpoint(epoch,model, optimizer, scheduler, save_cur=True)\n",
    "            else:\n",
    "                break\n",
    "            scheduler.step()\n",
    "            # if epoch == 15:\n",
    "            #     break\n",
    "    val_test.append(vals)\n",
    "    test_bests.append(tbest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "def sparse_diag_identity(n):\n",
    "    i = [i for i in range(n)]\n",
    "    return torch.sparse_coo_tensor(torch.tensor([i, i]), torch.ones(n))\n",
    "def sparse_diag(tensor):\n",
    "    i = [i for i in range(tensor.shape[0])]\n",
    "    return torch.sparse_coo_tensor(torch.tensor([i, i]), tensor)\n",
    "\n",
    "l = pickle.load(open('/home/justin/Mesh/Matrices_out/{}_lvl_{}.pkl'.format('l',6), \"rb\"))\n",
    "\n",
    "L1 = torch.abs(l['L1d'] + l['L1u'])\n",
    "L2 = torch.abs(l['L2d'])\n",
    "L0 = torch.abs(l['L0u'])\n",
    "# E = l['E']\n",
    "# Fa = l['F']\n",
    "B1 = l[\"B1\"]\n",
    "B1t = l[\"B1t\"]\n",
    "B2 = l[\"B2\"]\n",
    "B2t = l[\"B2t\"]\n",
    "\n",
    "x0 = 40962\n",
    "x1 = 122880\n",
    "x2 = 81920\n",
    "\n",
    "B1_v_abs, B1_i = torch.abs(B1.coalesce().values()), B1.coalesce().indices()\n",
    "B1_sum = torch.sparse.sum(torch.sparse_coo_tensor(B1_i, B1_v_abs, (x0, x1)), dim=1)\n",
    "B1_sum_values = B1_sum.to_dense()\n",
    "B1_sum_indices = torch.tensor([i for i in range(x0)])\n",
    "d0_diag_indices = torch.stack([B1_sum_indices, B1_sum_indices], dim=0)\n",
    "d0 = torch.sparse_coo_tensor(d0_diag_indices, B1_sum_values, (x0, x0))\n",
    "B1_sum_inv_values = torch.nan_to_num(1. / B1_sum_values, nan=0., posinf=0., neginf=0.)\n",
    "d0_inv = torch.sparse_coo_tensor(d0_diag_indices, B1_sum_inv_values, (x0, x0))\n",
    "L0 = torch.sparse.mm(L0, d0_inv)\n",
    "L0_factor_values = -1 / (B1_sum_inv_values + 1)\n",
    "L0_factor = torch.sparse_coo_tensor(d0_diag_indices, L0_factor_values, (x0, x0))\n",
    "L0_bias_values = torch.ones(d0.shape[0])\n",
    "L0bias = torch.sparse_coo_tensor(d0_diag_indices, L0_bias_values, (x0, x0))\n",
    "L0 = L0bias + torch.sparse.mm(L0, L0_factor)\n",
    "\n",
    "D1_inv = torch.sparse_coo_tensor(d0_diag_indices, 0.5 * B1_sum_inv_values, (x0, x0))\n",
    "B2_v_abs, B2_i = torch.abs(B2.coalesce().values()), B2.coalesce().indices()\n",
    "D2diag_1 = torch.sparse.sum(torch.sparse_coo_tensor(B2_i, B2_v_abs, (x1, x1)), dim=1).to_dense()\n",
    "D2diag = torch.maximum(D2diag_1, torch.ones(D2diag_1.shape[0]))\n",
    "D2_indices = [i for i in range(D2diag.shape[0])]\n",
    "D2_indices = torch.tensor([D2_indices, D2_indices])\n",
    "D2 = torch.sparse_coo_tensor(D2_indices, D2diag, (x1, x1))\n",
    "D2_inv = torch.sparse_coo_tensor(D2_indices, 1 / D2diag, (x1, x1))\n",
    "D3_values = (1 / 3.) * torch.ones(B2.shape[1])\n",
    "D3_indices = [i for i in range(B2.shape[1])]\n",
    "D3_indices = torch.tensor([D3_indices, D3_indices])\n",
    "D3 = torch.sparse_coo_tensor(D3_indices, D3_values, (x2, x2))\n",
    "A_1u = D2 - torch.sparse.mm(torch.sparse.mm(B2, D3), B2.t())\n",
    "A_1d = D2_inv - torch.sparse.mm(torch.sparse.mm(B1.t(), D1_inv), B1)\n",
    "A_1u_norm = torch.sparse.mm((sparse_diag_identity(A_1u.shape[0]) + A_1u),\n",
    "                                    (torch.sparse_coo_tensor(D2_indices, 1 / (D2diag + 1), (x1, x1))))\n",
    "A_1d_norm = torch.sparse.mm((D2 + sparse_diag_identity(D2.shape[0])),\n",
    "                                    (A_1d + sparse_diag_identity(A_1d.shape[0])))\n",
    "L1 = A_1u_norm + A_1d_norm\n",
    "\n",
    "B2_sum = D2diag_1\n",
    "B2_sum_inv = 1 / (B2_sum + 1)\n",
    "D5inv = sparse_diag(B2_sum_inv)\n",
    "A_2d = sparse_diag_identity(B2.shape[1]) + torch.sparse.mm(torch.sparse.mm(B2.t(), D5inv), B2)\n",
    "A_2d_norm = torch.sparse.mm((2 * sparse_diag_identity(B2.shape[1])),\n",
    "                                    (A_2d + sparse_diag_identity(A_2d.shape[0])))\n",
    "L2 = A_2d_norm\n",
    "\n",
    "B2D3 = torch.sparse.mm(B2, D3)\n",
    "D2B1TD1inv = (1 / np.sqrt(2.)) * torch.sparse.mm(torch.sparse.mm(D2, B1.t()), D1_inv)\n",
    "D1invB1 = (1 / np.sqrt(2.)) * torch.sparse.mm(D1_inv, B1)\n",
    "B2TD2inv = torch.sparse.mm(B2.t(), D5inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys; \n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "def sparse_diag_identity(n):\n",
    "    i = [i for i in range(n)]\n",
    "    return torch.sparse_coo_tensor(torch.tensor([i, i]), torch.ones(n))\n",
    "def sparse_diag(tensor):\n",
    "    i = [i for i in range(tensor.shape[0])]\n",
    "    return torch.sparse_coo_tensor(torch.tensor([i, i]), tensor)\n",
    "class MaxPool(nn.Module):\n",
    "    def __init__(self, level, nv_prev):\n",
    "        super().__init__()\n",
    "        self.level = level\n",
    "        self.nv_prev = nv_prev\n",
    "\n",
    "        if self.level > 0:\n",
    "\n",
    "            neihboring_patches_file = os.path.join(\"./Matrices_out/l_%d_patches.npy\" % level)\n",
    "            self.neihboring_patches = np.load(neihboring_patches_file)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 0.5 * torch.max(x[:, :, self.neihboring_patches], -1)[0] + x[...,:self.nv_prev] * 0.5\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class DownSamp(nn.Module):\n",
    "    def __init__(self, nv_prev, lvl, in_chan,hami,e2n = False, down = True):\n",
    "        super().__init__()\n",
    "        self.nv_prev = nv_prev\n",
    "        self.lvl = lvl + 1\n",
    "        self.e2n = e2n\n",
    "        self.down = down\n",
    "        if e2n:\n",
    "            if down:\n",
    "                #face-lvel \n",
    "                self.e2f = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "                self.ff = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "                self.f = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "\n",
    "                #edge-level\n",
    "                self.ee1 = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "                self.ee2 = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "                self.f2e = nn.Linear(2*in_chan, 2*in_chan, bias=True)\n",
    "                self.n2e = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "                self.e = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "\n",
    "                #node-level\n",
    "                self.nn = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "                self.e22n = nn.Linear(2*in_chan, 2*in_chan, bias=True)\n",
    "                self.n = nn.Linear(in_chan, 2*in_chan, bias=True)\n",
    "\n",
    "                self.affine_a = nn.Parameter(torch.ones([1,2*in_chan, 1]))\n",
    "                self.affine_b = nn.Parameter(torch.zeros([1,2*in_chan, 1]))\n",
    "                \n",
    "                self.f_bn1 = nn.BatchNorm1d(2*in_chan)\n",
    "                self.e_bn1 = nn.BatchNorm1d(2*in_chan)\n",
    "                self.n_bn1 = nn.BatchNorm1d(2*in_chan)\n",
    "\n",
    "                self.pool = MaxPool(lvl,(30 * (4 ** lvl) - 20 * (4**lvl) + 2))\n",
    "\n",
    "            else:\n",
    "                self.e2f = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.ff = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.f = nn.Linear(in_chan, in_chan, bias=True)\n",
    "\n",
    "                #edge-level\n",
    "                self.ee1 = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.ee2 = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.f2e = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.n2e = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.e = nn.Linear(in_chan, in_chan, bias=True)\n",
    "\n",
    "                #node-level\n",
    "                self.nn = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.e22n = nn.Linear(in_chan, in_chan, bias=True)\n",
    "                self.n = nn.Linear(in_chan, in_chan, bias=True)\n",
    "\n",
    "                self.affine_a = nn.Parameter(torch.ones([1,in_chan, 1]))\n",
    "                self.affine_b = nn.Parameter(torch.zeros([1,in_chan, 1]))\n",
    "\n",
    "                self.f_bn1 = nn.BatchNorm1d(in_chan)\n",
    "                self.e_bn1 = nn.BatchNorm1d(in_chan)\n",
    "                self.n_bn1 = nn.BatchNorm1d(in_chan)\n",
    "        \n",
    "        self.hami = hami\n",
    "        self.ch = in_chan\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.e2n is True:\n",
    "\n",
    "            l = pickle.load(open('/home/justin/Mesh/Mesh/Norm_M/{}_lvl_{}.pkl'.format(self.hami,self.lvl), \"rb\"))\n",
    "\n",
    "            E = l['E']\n",
    "            Fa = l['F']\n",
    "           \n",
    "            L1d = torch.abs(l['L1dn']).cuda()\n",
    "            L1u = torch.abs(l['L1un']).cuda()\n",
    "            L2d = torch.abs(l['L2dn']).cuda()\n",
    "            L0u = torch.abs(l['L0un']).cuda()\n",
    "            B2D3 = torch.abs(l['B2D3']).cuda()\n",
    "            D2B1TD1inv= torch.abs(l[ 'D2B1TD1inv']).cuda()\n",
    "            D1invB1= torch.abs(l['D1invB1']).cuda()\n",
    "            B2TD2inv= torch.abs(l['B2TD2inv']).cuda()\n",
    "            L0 = torch.abs(L0u.cuda())\n",
    "            L2 = torch.abs(L2d.cuda())\n",
    "\n",
    "            batch_size = x.shape[0]\n",
    "            edge_features_list = []\n",
    "            # PE_list = []\n",
    "            for b in range(batch_size):\n",
    "                concatenated_features = ( 0.5 * x[b,:,E[:,0]].unsqueeze(0) ) +  ( 0.5 * x[b,:,E[:,1]].unsqueeze(0) )\n",
    "                concatenated_features = concatenated_features.squeeze()\n",
    "                edge_features_list.append(concatenated_features)\n",
    "\n",
    "            edge_features = torch.stack(edge_features_list, dim=0)\n",
    "\n",
    "            face_features_list = []\n",
    "            PE_list = []\n",
    "            Fa= torch.tensor(Fa)\n",
    "            for b in range(batch_size):\n",
    "\n",
    "                concatenated_features =( 0.3 * x[b,:,Fa[:,0]].unsqueeze(0) ) + (0.3 * x[b,:,Fa[:,1]].unsqueeze(0)) + (0.3 * x[b,:,Fa[:,2]].unsqueeze(0))\n",
    "                concatenated_features = concatenated_features.squeeze()\n",
    "                face_features_list.append(concatenated_features)\n",
    "\n",
    "            face_features = torch.stack(face_features_list, dim=0)\n",
    "            FF  =self.ff(face_features.permute(0,2,1)).permute(0,2,1)\n",
    "            FF = spmatmul(FF, L2.double())\n",
    "\n",
    "            E2F = self.e2f(edge_features.permute(0,2,1)).permute(0,2,1)\n",
    "            E2F = spmatmul(E2F, B2TD2inv.double())\n",
    "        \n",
    "            face_features = (1/2.) * F.leaky_relu(self.f_bn1((FF + E2F +self.f(face_features.permute(0,2,1)).permute(0,2,1)).float()))\n",
    "            #edge-level\n",
    "            EE1  =self.ee1(edge_features.permute(0,2,1)).permute(0,2,1)\n",
    "            EE1 = spmatmul(EE1, L1d.double())\n",
    "            \n",
    "            EE2  =self.ee2(edge_features.permute(0,2,1)).permute(0,2,1)\n",
    "            EE2 = spmatmul(EE2, L1u.double())\n",
    "            \n",
    "            F2E = self.f2e(face_features.permute(0,2,1)).permute(0,2,1)\n",
    "            F2E= spmatmul(F2E, B2D3.double())\n",
    "\n",
    "            N2E = self.n2e(x.permute(0,2,1)).permute(0,2,1)\n",
    "            N2E= spmatmul(N2E, D2B1TD1inv.double())\n",
    "\n",
    "            edge_features = (1/3.)*F.leaky_relu(self.e_bn1((EE1 + EE2 + F2E + N2E +self.e(edge_features.permute(0,2,1)).permute(0,2,1)).float()))\n",
    "\n",
    "            #node-level\n",
    "            NN  =self.nn(x.permute(0,2,1)).permute(0,2,1)\n",
    "            NN = spmatmul(NN, L0.double())\n",
    "\n",
    "            E2N = self.e22n(edge_features.permute(0,2,1)).permute(0,2,1)\n",
    "            E2N = spmatmul(E2N, D1invB1.double())\n",
    "\n",
    "\n",
    "            final_node = (1/2.) * F.leaky_relu(self.n_bn1(self.affine_a*(NN + E2N + self.n(x.permute(0,2,1)).permute(0,2,1)).float()+self.affine_b) )\n",
    "            if self.down:\n",
    "                return self.pool(final_node)\n",
    "\n",
    "            else:\n",
    "                return final_node.float()\n",
    "\n",
    "\n",
    "        return x[..., :self.nv_prev] \n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_chan, neck_chan, out_chan, level, coarsen, mesh_folder, e2n = False):\n",
    "        super().__init__()\n",
    "\n",
    "        in_chan = int(in_chan//2)\n",
    "        neck_chan = int(neck_chan//2)\n",
    "        out_chan = int(out_chan//2)\n",
    "\n",
    "        l = level-1 if coarsen else level\n",
    "        self.coarsen = coarsen\n",
    "        mesh_file = os.path.join(mesh_folder, \"left_icosphere_{}.pkl\".format(l))\n",
    "        mesh_file2 = os.path.join(mesh_folder, \"right_icosphere_{}.pkl\".format(l))\n",
    "        self.conv2 = MeshConv(neck_chan, neck_chan, mesh_file,mesh_file2, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.nv_prev = self.conv2.l_nv_prev\n",
    "        self.down1 = DownSamp(self.nv_prev,l,in_chan,'l',e2n, down = False)\n",
    "        self.down11 = DownSamp(self.nv_prev,l,in_chan,'l',e2n, down = False)\n",
    "        self.down111 = DownSamp(self.nv_prev,l,in_chan,'l',e2n)\n",
    "        self.down3 = DownSamp(self.nv_prev,l,in_chan,'r',e2n, down = False)\n",
    "        self.down33 = DownSamp(self.nv_prev,l,in_chan,'r',e2n, down = False)\n",
    "        self.down333 = DownSamp(self.nv_prev,l,in_chan,'r',e2n)\n",
    "\n",
    "        self.diff_chan = (in_chan != out_chan)\n",
    "\n",
    "        if coarsen:\n",
    "            self.seq1 = nn.Sequential(self.down1, self.down11, self.down111)\n",
    "\n",
    "        if coarsen:\n",
    "            self.r_seq1 = nn.Sequential(self.down3,self.down33,self.down333)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        dim= x.shape[1]\n",
    "        l_x,r_x  = x[:,:int(dim/2),:],x[:,int(dim/2):,:]\n",
    "\n",
    "        x1 = self.seq1(l_x)\n",
    "        r_x1 = self.r_seq1(r_x)\n",
    "\n",
    "        x1 = torch.cat([x1,r_x1],axis = 1)\n",
    "\n",
    "        out = x1 \n",
    "        \n",
    "        return out\n",
    "class ConvBNReLU1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, bias=True, activation='relu'):\n",
    "        super(ConvBNReLU1D, self).__init__()\n",
    "        # self.act = nn.ReLU(inplace=True)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, bias=bias),\n",
    "            # nn.BatchNorm1d(out_channels),\n",
    "            # self.act\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, mesh_folder, feat=16, nclasses=10):\n",
    "        super().__init__()\n",
    "        mf_l = os.path.join(mesh_folder, \"left_icosphere_6.pkl\")\n",
    "        mf_r = os.path.join(mesh_folder, \"right_icosphere_6.pkl\")\n",
    "\n",
    "        self.in_conv = MeshConv(8, 2*feat, l_mesh_file=mf_l,r_mesh_file=mf_r, stride=1, agg = False)\n",
    "        self.in_bn = nn.BatchNorm1d(2*feat)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.in_block = nn.Sequential(self.in_conv, self.in_bn, self.relu)\n",
    "\n",
    "        self.maxp = MaxPool(5, 10242)\n",
    "        self.block2 = ResBlock(in_chan=2*feat, neck_chan= 4*feat, out_chan=4*feat, level=5, coarsen=True, mesh_folder=mesh_folder, e2n = True)\n",
    "        self.block3 = ResBlock(in_chan=4*feat, neck_chan= 8*feat, out_chan=8*feat, level=4, coarsen=True, mesh_folder=mesh_folder, e2n = True)\n",
    "        self.block4 = ResBlock(in_chan=8*feat, neck_chan=16*feat, out_chan=16*feat, level=3, coarsen=True, mesh_folder=mesh_folder, e2n = True)\n",
    "        self.block5 = ResBlock(in_chan=16*feat, neck_chan=32*feat, out_chan=32*feat, level=2, coarsen=True, mesh_folder=mesh_folder, e2n = True)\n",
    "\n",
    "        self.avg = nn.AvgPool1d(kernel_size=self.block5.nv_prev) # output shape batch x channels x 1\n",
    "        L_relu = nn.LeakyReLU()\n",
    "        self.out_layer= nn.Sequential(nn.Linear(32*feat,16*feat),L_relu,nn.Linear(16*feat,8*feat),L_relu,nn.Linear(8*feat,nclasses)) #nn.Softmax(dim=0) for multi-class\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.in_block(x)\n",
    "        x = self.maxp(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "\n",
    "        x_l = torch.squeeze(self.avg(x[:, :x.shape[1]//2,: ]))\n",
    "        x_r = torch.squeeze(self.avg(x[:, x.shape[1]//2:,: ]))\n",
    "        x= torch.cat([x_l,x_r], dim = 1)\n",
    "        x = self.out_layer(x)\n",
    "\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "import torch\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "random_seed = 55\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "\n",
    "\n",
    "#Load data set X and y\n",
    "#X is the cortical surface thickness data [N, 40962, 2] where first channel is the left/right hemisphere, second channel is the other hemisphere.\n",
    "#y is the label (AD/NC or AGE) [N,1] \n",
    "\n",
    "k = 5     # k-cross validation에서 k의 값이 5\n",
    "test_bests = []\n",
    "val_test= []\n",
    "skf = StratifiedShuffleSplit(n_splits=k, test_size = 0.2, train_size = 0.8, random_state = 42)\n",
    "for i, (trainval_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "    # Split data into training and test sets for the current fold\n",
    "    X_trainval, X_test = X[trainval_idx], X[test_idx]\n",
    "    y_trainval, y_test = y[trainval_idx], y[test_idx]\n",
    "    \n",
    "    # Further split the training set into training and validation sets\n",
    "    print(len(y_test), len(X))\n",
    "    skf = StratifiedShuffleSplit(n_splits=1, test_size = 0.1, train_size = 0.9)\n",
    "    for i, (train_idx, val_idx) in enumerate(skf.split(X_trainval, y_trainval)):\n",
    "        X_train, X_val = X_trainval[train_idx], X_trainval[val_idx]\n",
    "        y_train, y_val = y_trainval[train_idx], y_trainval[val_idx]\n",
    "\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    train_list = []\n",
    "    for i,j in enumerate(list(X_train)):\n",
    "        train_list.append([j,list(y_train)[i]])\n",
    "\n",
    "    val_list = []\n",
    "    for i,j in enumerate(list(X_val)):\n",
    "        val_list.append([j,list(y_val)[i]])\n",
    "\n",
    "\n",
    "    test_list = []\n",
    "    for i,j in enumerate(list(X_test)):\n",
    "        test_list.append([j,list(y_test)[i]])\n",
    "\n",
    "    print(len(train_list), len(val_list), len(test_list))\n",
    "    train_loader = DataLoader(train_list, batch_size = 32, shuffle = True, drop_last = True)\n",
    "    val_loader = DataLoader(val_list, batch_size = len(val_list))\n",
    "    test_loader = DataLoader(test_list, batch_size = len(test_list))\n",
    "\n",
    "\n",
    "    # train_loader\n",
    "    train_features, train_labels = next(iter(train_loader))\n",
    "    print(f\"Feature batch shape: {train_features.size()}\")\n",
    "    print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "    val_features, val_labels = next(iter(val_loader))\n",
    "    print(f\"Feature batch shape: {val_features.size()}\")\n",
    "    print(f\"Labels batch shape: {val_labels.size()}\")\n",
    "\n",
    "    test_features, test_labels = next(iter(test_loader))\n",
    "    print(f\"Feature batch shape: {test_features.size()}\")\n",
    "    print(f\"Labels batch shape: {test_labels.size()}\")\n",
    "\n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "    import torch.optim as optim\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "    %cd /home/justin/Mesh\n",
    "    #Model parameters: template path, feature dimension, output class number\n",
    "    model = Model(\".\",16, 2)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    logger = logging.getLogger(\"train\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.handlers = []\n",
    "    ch = logging.StreamHandler()\n",
    "    logger.addHandler(ch)\n",
    "    fh = logging.FileHandler(os.path.join('.', \"absolute_aggregation_16.txt\"))\n",
    "    logger.addHandler(fh)\n",
    "    test_acc = None\n",
    "    logger.info(\"{} paramerters in total\".format(sum(x.numel() for x in model.parameters())))\n",
    "    best = []\n",
    "    tbest = []\n",
    "    best_loss = 100\n",
    "    tolerance = 0\n",
    "\n",
    "    for epoch in range(1, 40 + 1):\n",
    "            logger.info(\"[Epoch {}]\".format(epoch))\n",
    "            train(model, device, train_loader, optimizer, epoch, logger)\n",
    "            best, acc, val_loss= val( model, device, val_loader, logger,best)\n",
    "            if val_loss <= best_loss:\n",
    "                best_loss = val_loss\n",
    "                tolerance = 0\n",
    "            else:\n",
    "                tolerance += 1\n",
    "            print(tolerance)\n",
    "            if acc == max(best):\n",
    "                    t_best, test_acc= test( model, device, test_loader, logger, tbest, append = True)\n",
    "                    vals = [acc, test_acc]\n",
    "            else:\n",
    "                    t_best, test_acc= test( model, device, test_loader, logger, tbest, append = False)\n",
    "            if epoch <= 10:\n",
    "                tolerance = 0\n",
    "                \n",
    "            if tolerance <= 8:\n",
    "                # break\n",
    "                save_checkpoint(epoch,model, optimizer, scheduler, save_cur=True)\n",
    "            else:\n",
    "                break\n",
    "            scheduler.step()\n",
    "    val_test.append(vals)\n",
    "    test_bests.append(tbest)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ends at 35, stops at 9:  91.4634 + 89.0244 + 93.9024 + 89.0244 + 92.6829 = 91.2195\n",
    "\n",
    "\n",
    "0.8378378378378378 + 0.7837837837837838 + 0.8648648648648649 + 0.8648648648648649 + 0.8918918918918919 = 0.8486486486486486\n",
    "\n",
    "\n",
    "0.9777777777777777+ 0.9777777777777777 + 1.0 + 0.9111111111111111 + 0.9555555555555556 = 0.9644444444444444"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ugsnn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 16:01:55) \n[GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e798e29d53ac39f7f81993fb436f798c318e8725211e9588680039ac19558076"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
